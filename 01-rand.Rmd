# Methods for Generating Random Variables {#rvar}

## Introduction

Most of the methods so-called *computational statistics* requires generation of random variables from specified probability distribution. In hand, we can spin wheels, roll a dice, or shuffle cards. The results are chosen randomly. However, we want the same things with computer. Here, `r`. As we know, computer cannot generate complete uniform random numbers. Instead, we generate **pseudo-random** numbers.

## Pseudo-random Numbers

```{definition, name = "Pseudo-random numbers"}
Sequence of values generated deterministically which have all the appearances of being independent $unif(0, 1)$ random variables, i.e.

$$x_1, x_2, \ldots, x_n \stackrel{iid}{\sim} unif(0, 1)$$
```


- behave *as if* following $unif(0, 1)$
- typically generated from an *initial seed*

### Linear congruential generator

<!-- Let $x_0, x_1, \ldots, x_n \in \mathbb{Z}_{+}$. -->

<!-- 1. Set $x_0$ as initial seed. -->
<!-- 2. Generate $x_n, n = 1, 2, \ldots$ recursively: -->
<!--     a. $x_n = (a x_{n - 1} + c) \mod m$ -->
<!--     b. where $a, c \in \mathbb{Z}_{+}, m: \text{modulus}$ -->
<!-- 3. Compute $u_n = \frac{x_n}{m} \in (0, 1)$ -->

Then $u_1, u_2, \ldots, u_n \sim unif(0, 1)$

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{$x_0, x_1, \ldots, x_n \in \mathbb{Z}_{+}$}
  Initialize $x_0$\;
  \For{$i \leftarrow 1$ \KwTo $n$}{
    $x_i = (a x_{i - 1} + c) \mod m$
    where $a, c \in \mathbb{Z}_{+}, m: \text{modulus}$\;
  }
  $u_i = \frac{x_i}{m} \in (0, 1)$\;
  \Output{$u_1, u_2, \ldots, u_n \sim unif(0, 1)$}
  \caption{Linear congruential generator}
\end{algorithm}

```{r}
lcg <- function(n, seed, a, b, m) {
  x <- rep(seed, n + 1)
  for (i in 1:n) {
    x[i + 1] <- (a * x[i] + b) %% m
  }
  x[-1] / m
}
```

```{r}
tibble(
  x = lcg(1000, 0, 1664525, 1013904223, 2^32)
) %>% 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 30, col = gg_hcl(1))
```


### Multiplicative congruential generator

As we can expect from its name, this is congruential generator with $c = 0$.

<!-- 1. Set $x_0$ as initial seed. -->
<!-- 2. Generate $x_n, n = 1, 2, \ldots$ recursively: -->
<!--     a. $x_n = a x_{n - 1} \mod m$ -->
<!--     b. where $a \in \mathbb{Z}_{+}, m: \text{modulus}$ -->
<!-- 3. Compute $u_n = \frac{x_n}{m} \in (0, 1)$ -->

<!-- Then $u_1, u_2, \ldots, u_n \sim unif(0, 1)$ -->

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{$x_0, x_1, \ldots, x_n \in \mathbb{Z}_{+}$}
  Initialize $x_0$\;
  \For{$i \leftarrow 1$ \KwTo $n$}{
    $x_i = a x_{i - 1} \mod m$
    where $a \in \mathbb{Z}_{+}, m: \text{modulus}$\;
  }
  $u_i = \frac{x_i}{m} \in (0, 1)$\;
  \Output{$u_1, u_2, \ldots, u_n \sim unif(0, 1)$}
  \caption{Multiplicative congruential generator}
\end{algorithm}

We just set `b = 0` in our `lcg()` function. The **seed must not be zero**.

```{r}
tibble(
  x = lcg(1000, 5, 1664525, 0, 2^32)
) %>% 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 30, col = gg_hcl(1))
```

### Cycle

Generate LCG $n = 32$ with $a = 1$, $c = 1$, and $m = 16$ from the seed $x_0 = 0$.

```{r}
lcg(32, 0, 1, 1, 16)
```

Observe that we have the cycle after $m$-th number. Against this problem, we give different seed from every $(im + 1)$th random number.


## The Inverse Transform Method

```{definition, icdf, name = "Inverse of CDF"}
Since some cdf $F_X$ is not strictly increasing, we difine $F_X^{-1}(y)$ for $0 < y < 1$ by

$$F_{X}^{-1}(y) := inf \{ x : F_X(x) \ge y \}$$
```

Using this definition, we can get the following theorem.

```{theorem, probint, name = "Probability Integral Transformation"}
If $X$ is a continuous random variable with cdf $F_(x)$, then
$$U \equiv F_X(X) \sim unif(0, 1)$$
```

```{proof, name = "Probability Integral Transformation"}
Let $U \sim unif(0, 1)$. Then

\begin{equation*}
  \begin{split}
    P(F_X^{-1}(U) \le x) & = P(\inf\{t : F_X(t) = U \} \le x) \\
    & = P(U \le F_X(x)) \\
    & = F_U(F_X(x)) \\
    & = F_X(x)
  \end{split}
\end{equation*}
```

Thus, to generate $n$ random variables $\sim F_X$,

<!-- 1. form of $F_X^{-1}(u)$ -->
<!-- 2. For each $i = 1, 2, \ldots, n$: -->
<!--     a. Generate $u_i \sim unif(0, 1)$ -->
<!--     b. $x_i = F_X^{-1}(u_i)$ -->

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{analytical form of $F_X^{-1}$}
  \For{$i \leftarrow 1$ \KwTo $n$}{
    $u_i \iid unif(0, 1)$\;
    $x_i = F_X^{-1}(u_i)$\;
  }
  \Output{$x_1, x_2, \ldots, x_n \iid F_X$}
  \caption{Inverse transformation method}
\end{algorithm}

### Continuous case

Denote that the *probability integral transformation* holds for a continuous variable. When generating continuous random variable, applying above algorithm might work.

```{example, expon, name = "Exponential distribution"}
If $X \sim Exp(\lambda)$, then $F_X(x) = 1 - e^{-\lambda x}$. We can derive the inverse function of cdf
$$F_X^{-1}(u) = \frac{1}{\lambda}\ln(1 - u)$$
```

Note that

$$U \sim unif(0, 1) \Leftrightarrow 1 - U \sim unif(0, 1)$$

Then we just can use $U$ instead of $1 - U$.

```{r}
inv_exp <- function(n, lambda) {
  -log(runif(n)) / lambda
}
```

If we generate $x_1, \ldots, x_{500} \sim Exp(\lambda = 1)$,

```{r cdfexp, fig.cap="Inverse Transformation: Exp(1)"}
gg_curve(dexp, from = 0, to = 10) +
  geom_histogram(
    data = tibble(x = inv_exp(500, lambda = 1)),
    aes(x = x, y = ..density..),
    bins = 30,
    fill = gg_hcl(1),
    alpha = .5
  )
```


### Discrete case

<!-- 1. For each $i = 1, 2, \ldots, n$: -->
<!--     a. Generate $u_i \sim unif(0, 1)$ -->
<!--     b. Take $x_i$ s.t. $F_X(x_{i - 1}) < U \le F_X(x_i)$ -->

<!-- Collect $x_1, x_2, \ldots, x_n \sim F_X$. -->

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{analytical form of $F_X$}
  \For{$i \leftarrow 1$ \KwTo $n$}{
    $u_i \iid unif(0, 1)$\;
    Take $x_i$ s.t. $F_X(x_{i - 1}) < U \le F_X(x_i)$\;
  }
  \Output{$x_1, x_2, \ldots, x_n \iid F_X$}
  \caption{Inverse transformation method in discrete case}
\end{algorithm}

```{r}
pmf <-
  tibble(
    x = 0:4,
    p = c(.1, .2, .2, .2, .3)
  )
```

```{r, exdis, echo=FALSE}
pmf %>% 
  t() %>% 
  knitr::kable(format = "pandoc", col.names = NULL, caption = "Example of a Discrete Random Variable")
```

```{example, dismass, name = "Discrete Random Variable"}
Consider a discrete random variable $X$ with a mass function as in Table \@ref(tab:exdis).
```

i.e.

```{r massfun, echo=FALSE, fig.cap="Probability Mass Function"}
pmf %>% 
  ggplot() +
  geom_segment(aes(x = x, y = 0, xend = x, yend = p)) +
  ylab(expression(p(x)))
```

Then we have the cdf

```{r cdfun, echo=FALSE, fig.cap="CDF of the Discrete Random Variable: Illustration for discrete case"}
pmf %>% 
  mutate(
    fx = cumsum(p),
    x_end = lead(x, default = 5),
    u = fx,
    u = ifelse(u == .5, .6, u),
    fx1 = lead(fx, default = 1),
    rand = u > fx & u <= fx1
  ) %>% 
  ggplot() +
  geom_segment(aes(x = x, y = fx, xend = x_end, yend = fx)) +
  ylab(expression(F(x))) +
  geom_segment(
    aes(x = 0, y = u, xend = x_end, yend = u, colour = rand),
    linetype = "dashed",
    arrow = arrow(length = unit(.5, "cm")),
    show.legend = FALSE
  ) +
  geom_segment(
    aes(x = x_end, y = u, xend = x_end, yend = 0, colour = rand),
    linetype = "dashed",
    arrow = arrow(length = unit(.5, "cm")),
    show.legend = FALSE
  ) +
  scale_colour_manual(
    values = c("TRUE" = gg_hcl(1), "FALSE" = "#00000000")
  )
```


Remembering the algorithm, we can implement `dplyr::case_when()` here.

```{r}
rcustom <- function(n) {
  tibble(u = runif(n)) %>% 
    mutate(
      x = case_when(
        u > 0 & u <= .1 ~ 0,
        u > .1 & u <= .3 ~ 1,
        u > .3 & u <= .5 ~ 2,
        u > .5 & u <= .7 ~ 3,
        TRUE ~ 4
      )
    ) %>% 
    select(x) %>% 
    pull()
}
```

```{r randmass, fig.cap="Generated discrete random numbers"}
tibble(
  x = rcustom(100)
) %>% 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = ..ndensity..), binwidth = .1)
```

See Figure \@ref(fig:massfun) and \@ref(fig:randmass). Comparing the two, the result can be said okay.

### Problems with inverse transformation

Examples \@ref(exm:expon) and \@ref(exm:dismass). We could generate these random numbers because we aware of

1. analytical $F_X$
2. $F^{-1}$

In practice, however, not all distribution have analytical $F$. Numerical computing might be possible, but it is not efficient. There are other approaches.


## The Acceptance-Rejection Method

Acceptance-rejection method does not require analytical form of cdf. What we need is our *target* density (or mass) function and *proposal* density (or mass) function. Target function is what we want to generate. Propsal function is of any random variable that is *easy to generate random numbers*. From this approach, we can generate any distribution while computation is not efficient.

|pdf or pmf|target or proposal|  
|:--------:|:--:|  
| $f$ | target|  
| $g$ | proposal - easy to generate random numbers |  

First of all, $g$ should satisfy that

$$spt f \subseteq spt g$$

Next, for some (pre-specified) $c > 0$

$$\forall x \in spt f : \frac{f(x)}{g(x)} \le c$$

### A-R algorithm

<!-- For $i = 1, \ldots, n$ -->

<!-- 1. $Y \sim g(Y)$ -->
<!-- 2. $U \sim unif(0, 1) \perp\!\!\!\perp Y$ -->
<!-- 3. Accept-Reject step -->
<!--     a. Accept: $U \le \frac{f(Y)}{cg(Y)} \Rightarrow x_i = Y$ -->
<!--     b. Reject: otherwise, go to step 1 -->

<!-- Collect $x_1, x_2, \ldots, x_n \stackrel{iid}{\sim} f(x)$. -->

\begin{algorithm}[H] \label{alg:aralg}
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{proposal density $g(x)$}
  \For{$i \leftarrow 1$ \KwTo $n$}{
    $Y \sim g(Y)$\;
    $U \sim unif(0, 1) \perp\!\!\!\perp Y$\;
    \eIf{$U \le \frac{f(Y)}{cg(Y)}$}{
      Accept $x_i = Y$\;
    }{
      go to line 2\;
    }
  }
  \Output{$x_1, x_2, \ldots, x_n \iid f(x)$}
  \caption{Acceptance-rejection algorithm}
\end{algorithm}

### Efficiency

```{r arprop, echo=FALSE, fig.cap="Property of AR method"}
illust_g <- function(x) {
  dbeta(x, shape1 = 3, shape2 = 2) * 2
}
#--------------------------------------
tibble(x = seq(0, 1, length.out = 101)) %>% 
  mutate(
    fx = dbeta(x, shape1 = 3, shape2 = 2),
    cgx = illust_g(x) - fx
  ) %>% 
  gather(-x, key = "density", value = "value") %>% 
  mutate(density = factor(density, levels = c("cgx", "fx"))) %>% 
  ggplot(aes(x = x)) +
  geom_area(aes(y = value, fill = density, colour = density), position = "stack", alpha = .5) +
  geom_segment(aes(x = .3, y = 0, xend = .3, yend = illust_g(.3)), linetype = "dashed") +
  geom_segment(aes(x = .3, y = 0, xend = .3, yend = dbeta(.3, shape1 = 3, shape2 = 2)), col = I("red")) +
  scale_fill_manual(
    values = c("fx" = NA, "cgx" = gg_hcl(1))
  ) +
  labs(
    x = "y",
    y = "density"
  )
```

See Figure \@ref(fig:arprop). This illustrates the motivation of A-R method. Lower one is $f(x)$ and the upper one is $cg(x)$ which covers $f$. We can see that

$$0 < \frac{f(x)}{cg(x)} \le 1$$

The algorithm takes random number from $Y \sim g$ in each recursive step $i$, which is represented as a line in the figure. At this value, the algorithm accept $Y$ as random number of $f$ if

$$U \le \frac{f(Y)}{cg(Y)}$$

Suppose that we choose a point at random on a line drawn in the figure \@ref(fig:arprop). If we get the red line, we accept. Otherwise, we reject. In other words, the *colored area is where we reject the given value*. The smaller the area is, the more efficient the algorithm will be.


```{proposition, arnote, name = "Properties of A-R Method"}
$\text{(1)}\: \frac{f(Y)}{cg(Y)} \perp\!\!\!\perp U$

$\text{(2)}\: 0 < \frac{f(x)}{cg(x)} \le 1$

$\text{(3)}\:$ Let $N$ be the number of iterations needed to get an acceptance. Then

$$N \sim Geo(p) \quad \text{where}\: p \equiv P\bigg(U \le \frac{f(Y)}{cg(Y)}\bigg)$$

and so

$$
\begin{cases}
  P(N = n) = p(1 - p)^{n - 1}I_{\{1, 2, \ldots \}}(n) \\
  E(N) = \text{average number of iterations} = \frac{1}{p}
\end{cases}
$$

$\text{(4)}\: X \sim Y \mid U \le \frac{f(Y)}{cg(Y)}$, i.e.

$$P\bigg(Y \le y \mid U \le \frac{f(Y)}{cg(Y)}\bigg) = F_X(y)$$
```

```{remark, areff, name = "Efficiency"}
Efficiency of the A-R method depends on $p = P\bigg(U \le \frac{f(Y)}{cg(Y)}\bigg)$. In fact,

$$E(N) = \frac{1}{p} = c$$

The algorithm becomes efficient for small $c$.
```

```{proof}
Note that

$$P\bigg( U \le \frac{f(y)}{cg(y)}, Y = y \bigg) = P\bigg(Y \le \frac{g(y)}{cg(y)} \mid Y = y \bigg)P(Y = y)$$

Since $U \sim unif(0, 1)$, $P\bigg(Y \le \frac{g(y)}{cg(y)} \mid Y = y \bigg) = \frac{f(y)}{cg(y)}$.

By construction, $P(Y = y) = g(y)$.

It follows that

\begin{equation*}
  \begin{split}
    p = P\bigg( U \le  \frac{f(y)}{cg(y)} \bigg) & = \int_{-\infty}^{\infty} P\bigg( U \le \frac{f(y)}{cg(y)}, Y = y \bigg) dy \\
    & = \int_{-\infty}^{\infty} \frac{f(y)}{cg(y)} g(y) dy \\
    & = \frac{1}{c} \int_{-\infty}^{\infty}f(y)dy \\
    & = \frac{1}{c}
  \end{split}
\end{equation*}

Hence,

$$E(N) = \frac{1}{p} = c$$

We can say that the method is efficient when the acceptance rate $p$ is large, i.e. $c$ small.
```

```{corollary, argood, name = "Efficiency of A-R Method"}
A-R method is efficient when

$g(\cdot)$ is close to $f(\cdot)$ and

have small $c$.
```

```{corollary, arc, name = "Choosing c"}
To enhance the algorithm, we might choose $c$ which satisfy

$$c = \max \bigg\{ \frac{f(x)}{g(x)} : x \in spt f \bigg\}$$
```

### Examples

```{example, arbeta, name = "Beta(a,b)"}
Let $X \sim Beta(a, b)$. Then the pdf of $X$ is given by

$$f(x) = \frac{1}{B(a, b)}x^{a - 1}(1 - x)^{b - 1}I_{(0, 1)}(x)$$
```

```{solution, arbetasol, name = "Generating Beta(a,b) with A-R method"}
Consider proposal density $g(x) = I_{(0, 1)}(x)$, i.e. $unif(0, 1)$.

To determine the optimal $c$ s.t.

$$c = \max \bigg\{ \frac{f(x)}{g(x)} : x \in (0, 1) \bigg\}$$

find the maximum of

$$\frac{f(x)}{g(x)} = \frac{1}{B(a, b)}x^{a - 1}(1 - x)^{b - 1}$$

Solve

\begin{equation*}
  \begin{split}
    \frac{d}{dx}\bigg(\frac{f(x)}{g(x)}\bigg) & = \frac{1}{B(a, b)}\Big( (a-1)x^{a-2}(1 - x)^{b - 1} - (b - 1)x^{a - 1}(1 - x)^{b - 2} \Big) \\
    & = \frac{x^{a - 2}(1 - x)^{b - 2}}{B(a, b)} \Big( (a - 1)(1 - x) - (b - 1)x \Big) \\
    & = \frac{x^{a - 2}(1 - x)^{b - 2}}{B(a, b)} \big( a - 1 - (a + b - 2)x \big) \quad = 0
  \end{split}
\end{equation*}

It follows that

$$\frac{f(x)}{g(x)} \le \frac{f(\frac{a - 1}{a + b - 2})}{g(\frac{a - 1}{a + b - 2})} = c$$

if $\frac{a - 1}{a + b - 2} \neq 0, 1$
```


```{r}
ar_beta <- function(n, a, b) {
  opt_x <- (a - 1) / (a + b - 2)
  opt_c <- dbeta(opt_x, shape1 = a, shape2 = b) / dunif(opt_x)
  X <- NULL
  N <- 0
  while (N <= n) {
    Y <- runif(n)
    U <- runif(n)
    X <- c(X, Y[U <= dbeta(Y, shape1 = a, shape2 = b) / opt_c])
    N <- length(X)
    if ( N > n ) X <- X[1:n]
  }
  X
}
```

Now we try to compare this A-R function to `R` `rbeta` function.

```{r}
gen_beta <-
  tibble(
    ar_rand = ar_beta(1000, 3, 2),
    sam = rbeta(1000, 3, 2)
  ) %>% 
  gather(key = "den", value = "value")
```


```{r betahis, fig.cap="Beta(3,2) Random numbers from each function"}
gg_curve(dbeta, from = 0, to = 1, args = list(shape1 = 3, shape2 = 2)) +
  geom_histogram(
    data = gen_beta,
    aes(x = value, y = ..density.., fill = den),
    position = "identity",
    bins = 30,
    alpha = .45
  ) +
  scale_fill_discrete(
    name = "random number",
    labels = c("AR", "rbeta")
  )
```

In the Figure \@ref(fig:betahis), the both histograms are very close to the true density curve. To see more statistically, we can draw a Q-Q plot.

```{r betaqq, fig.cap="Q-Q plot for Beta(3,2) random numbers"}
gen_beta %>% 
  ggplot(aes(sample = value)) +
  stat_qq_line(
    distribution = stats::qbeta,
    dparams = list(shape1 = 3, shape2 = 2),
    col = I("grey70"),
    size = 3.5
  ) +
  stat_qq(
    aes(colour = den),
    distribution = stats::qbeta,
    dparams = list(shape1 = 3, shape2 = 2)
  ) +
  scale_colour_discrete(
    name = "random number",
    labels = c("AR", "rbeta")
  )
```

See Figure \@ref(fig:betaqq). We have got series of numbers that are sticked to the beta distribution line.


```{example, ardiscrete, name = "A-R Method for Discrete case"}
A-R method can be also implemented to discrete case such as Example \@ref(exm:dismass).
```

```{r, exdis2, echo=FALSE}
pmf %>% 
  t() %>% 
  knitr::kable(format = "pandoc", col.names = NULL, caption = "Example of a Discrete Random Variable")
```

```{solution, ardiscretesol, name = "Generating discrete random numbers using A-R methods"}
Consider proposal $g(x) \sim \text{Discrete unif}(0, 1, 2, 3, 4)$, i.e.

$$g(0) = g(1) = \cdots = g(4) = 0.2$$

Then we set

$$c = \max\bigg\{ \frac{p(x)}{g(x)} : x = 0, \ldots, 4 \bigg\} = \max\Big\{ 0.5, 1, 1.5 \Big\} = 1.5$$
```


## Transfomation Methods


## Sums and Mixtures

### Sums

### Convolutions and mixtures

```{r, message=FALSE}
library(foreach)
```

```{r}
mix_norm <- function(n, p1, mean1, sd1, mean2, sd2) {
  x1 <- rnorm(n, mean = mean1, sd = sd1)
  x2 <- rnorm(n, mean = mean2, sd = sd2)
  k <- as.integer(runif(n) > p1)
  k * x1 + (1 - k) * x2
}
```

```{r}
mixture <-
  foreach(p1 = 0:10 / 10, .combine = bind_rows) %do% {
    tibble(
      value = mix_norm(n = 1000, p1 = p1, mean1 = 0, sd1 = 1, mean2 = 3, sd2 = 1),
      key = rep(p1, 1000)
    )
  }
```

```{r}
mixture %>% 
  ggplot(aes(x = value, colour = factor(key))) +
  stat_density(geom = "line", position = "identity") +
  scale_colour_discrete(
    name = expression(p[1]),
    labels = 0:10 / 10
  ) +
  xlab("x")
```


## Multivariate Normal Random Vector


## Stochastic Processes

### Homogeneous poisson process

### Nonhomogeneous poisson process

### Symmetric random walk











